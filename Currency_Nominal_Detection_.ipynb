{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNABadm8UbDjgcVotzmvimH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hbbalamsyah/Currency-Nominal-Detection-Using-CNN/blob/main/Currency_Nominal_Detection_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "NgsuNeXP1oJt",
        "outputId": "afbac555-5633-456a-b117-1521be206530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-2.13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jg3GjTzH0eAP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4kNjp7JP1IwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606a5240-c0e1-4fd8-cb51-b5e0edb50b01"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Dataset yang Telah di Preprocessing"
      ],
      "metadata": {
        "id": "T3WG5ZqRTcGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = '/content/drive/MyDrive/dataset uang baru/5. resize'"
      ],
      "metadata": {
        "id": "8pHRaDXS14dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelling"
      ],
      "metadata": {
        "id": "nau1CP6eTjM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained VGG16 model tanpa lapisan fully connected\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "\n",
        "# Tambahkan lapisan fully connected custom\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "output = Dense(7, activation='softmax')(x)\n",
        "\n",
        "# Buat model akhir dengan pre-trained VGG16 sebagai base\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Kunci lapisan base agar tidak ikut terlatih ulang\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Kompilasi model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "K9SE9Iis0pwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb297f33-7bae-4c81-a774-df8294136518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Model"
      ],
      "metadata": {
        "id": "YRG3PyNGTk1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing data menggunakan ImageDataGenerator\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # validation_split 20%\n",
        "\n",
        "# Pembuatan data generator untuk pelatihan\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'  # Gunakan data pelatihan\n",
        ")\n",
        "\n",
        "# Pembuatan data generator untuk validasi\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    dataset,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'  # Gunakan data validasi\n",
        ")\n",
        "\n",
        "# Pelatihan model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator\n",
        ")\n",
        "\n",
        "# Simpan model\n",
        "model.save('vgg16_rupiah_detection_model_epoch 10.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAoEy7TV03Ct",
        "outputId": "08b40419-5399-4daf-df20-b662001e8136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 896 images belonging to 7 classes.\n",
            "Found 224 images belonging to 7 classes.\n",
            "Epoch 1/10\n",
            "28/28 [==============================] - 879s 31s/step - loss: 2.8539 - accuracy: 0.2478 - val_loss: 1.6621 - val_accuracy: 0.2946\n",
            "Epoch 2/10\n",
            "28/28 [==============================] - 911s 33s/step - loss: 0.8958 - accuracy: 0.7254 - val_loss: 1.1398 - val_accuracy: 0.5804\n",
            "Epoch 3/10\n",
            "28/28 [==============================] - 928s 33s/step - loss: 0.4177 - accuracy: 0.9554 - val_loss: 0.8953 - val_accuracy: 0.7321\n",
            "Epoch 4/10\n",
            "28/28 [==============================] - 924s 33s/step - loss: 0.2197 - accuracy: 0.9944 - val_loss: 0.8332 - val_accuracy: 0.7188\n",
            "Epoch 5/10\n",
            "28/28 [==============================] - 927s 33s/step - loss: 0.1343 - accuracy: 1.0000 - val_loss: 0.7230 - val_accuracy: 0.7679\n",
            "Epoch 6/10\n",
            "28/28 [==============================] - 922s 33s/step - loss: 0.0846 - accuracy: 1.0000 - val_loss: 0.6626 - val_accuracy: 0.7857\n",
            "Epoch 7/10\n",
            "28/28 [==============================] - 896s 32s/step - loss: 0.0600 - accuracy: 1.0000 - val_loss: 0.6457 - val_accuracy: 0.7857\n",
            "Epoch 8/10\n",
            "28/28 [==============================] - 932s 34s/step - loss: 0.0443 - accuracy: 1.0000 - val_loss: 0.6046 - val_accuracy: 0.7991\n",
            "Epoch 9/10\n",
            "28/28 [==============================] - 935s 34s/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 0.5953 - val_accuracy: 0.7991\n",
            "Epoch 10/10\n",
            "28/28 [==============================] - 923s 33s/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.5870 - val_accuracy: 0.7902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mengambil labelling yang di dapat pada proses training**"
      ],
      "metadata": {
        "id": "AmMLBNuDMLi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah diambil labellingnya, simpan label ke dalam file H5, agar tidak perlu running ulang code"
      ],
      "metadata": {
        "id": "LfcU064bMVY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ambil label kelas dari generator pelatihan\n",
        "class_labels = list(train_generator.class_indices.keys())\n",
        "\n",
        "# Simpan label kelas ke dalam file H5\n",
        "h5_file_path = '/content/class_labels.h5'  # Sesuaikan dengan lokasi penyimpanan file H5\n",
        "with h5py.File(h5_file_path, 'w') as h5_file:\n",
        "    h5_file.create_dataset('class_labels', data=np.array(class_labels, dtype='S'))\n"
      ],
      "metadata": {
        "id": "0Wxiiw1untpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load file H5 Labelling yang telah disimpan"
      ],
      "metadata": {
        "id": "T85hd_OTNbRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load file H5 Labelling\n",
        "h5_file_path = '/content/drive/MyDrive/ujicoba/class_labels.h5'  # Sesuaikan path dengan lokasi file H5\n",
        "\n",
        "# Buat dataset untuk mendapatkan label kelas\n",
        "with h5py.File(h5_file_path, 'r') as h5_file:\n",
        "    class_labels = h5_file['class_labels'][:]  # Sesuaikan dengan nama dataset yang sesuai dalam file H5\n",
        "\n",
        "# Ubah data numpy menjadi list\n",
        "class_labels = class_labels.tolist()\n",
        "\n",
        "# Print label kelas\n",
        "print(\"Class Labels:\", class_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_RKef2GSCsB",
        "outputId": "ea6cae5d-2dc0-4454-e336-41522733282b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Labels: [b'100k', b'10k', b'1k', b'20k', b'2k', b'50k', b'5k']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import data uji**"
      ],
      "metadata": {
        "id": "wKfbwop8Mkbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import data uji\n",
        "\n",
        "dataset_uji_terang = '/content/drive/MyDrive/uang uji baru/Terang_125'\n",
        "dataset_uji_normal = '/content/drive/MyDrive/uang uji all'\n",
        "dataset_uji_gelap = '/content/drive/MyDrive/uang uji baru/Gelap_075'"
      ],
      "metadata": {
        "id": "wcQYY_2jCQZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing/Pengujian data all dalam 1 folder**"
      ],
      "metadata": {
        "id": "3mkFNKvEL38q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model yang telah dilatih\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/ujicoba/vgg16_rupiah_detection_model_epoch 5.h5')\n",
        "\n",
        "# Ambil label kelas dari generator pelatihan\n",
        "\n",
        "#class_labels = list(train_generator.class_indices.keys()) # Sudah diambil dari file berformat H5, jika belum ada baru di running\n",
        "\n",
        "# Direktori tempat menyimpan gambar-gambar uji\n",
        "dataset_uji_dir = dataset_uji_terang #ganti path jika mau uji dengan kondisi yang berbeda\n",
        "\n",
        "# List semua nama file gambar dalam direktori uji\n",
        "gambar_uji_filenames = os.listdir(dataset_uji_dir)\n",
        "\n",
        "# Lakukan prediksi pada setiap gambar uji\n",
        "for i, gambar_filename in enumerate(gambar_uji_filenames):\n",
        "    gambar_path = os.path.join(dataset_uji_dir, gambar_filename)\n",
        "    gambar = load_img(gambar_path, target_size=(256, 256))\n",
        "    gambar_array = img_to_array(gambar)\n",
        "    gambar_array = gambar_array.reshape((1, 256, 256, 3))\n",
        "    gambar_array /= 255.0  # Normalisasi intensitas\n",
        "    prediction = model.predict(gambar_array)\n",
        "    predicted_class = class_labels[prediction.argmax()]\n",
        "    print(f\"{gambar_filename} : : {predicted_class}\")"
      ],
      "metadata": {
        "id": "yvoCX8bv4nkf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9187ae3c-efd5-44ac-97b9-0903d1f0fa30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 763ms/step\n",
            "Copy of 1k4.JPG : : b'1k'\n",
            "1/1 [==============================] - 1s 620ms/step\n",
            "Copy of 2k4.JPG : : b'2k'\n",
            "1/1 [==============================] - 1s 610ms/step\n",
            "Copy of 5k4.JPG : : b'5k'\n",
            "1/1 [==============================] - 1s 608ms/step\n",
            "Copy of 10k4.JPG : : b'10k'\n",
            "1/1 [==============================] - 1s 603ms/step\n",
            "Copy of 20k4.JPG : : b'20k'\n",
            "1/1 [==============================] - 1s 606ms/step\n",
            "Copy of 50k4.JPG : : b'50k'\n",
            "1/1 [==============================] - 1s 614ms/step\n",
            "Copy of 100k4.JPG : : b'100k'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing/Pengujian data per folder**"
      ],
      "metadata": {
        "id": "rSAi1wxiLvSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model yang telah dilatih\n",
        "model = tf.keras.models.load_model('/content/vgg16_rupiah_detection_model_epoch 5.h5')\n",
        "\n",
        "# Preprocessing data uji menggunakan ImageDataGenerator\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Pembuatan data generator untuk uji\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    dataset_uji_terang,  # Ganti dengan path sesuai direktori data mana yang akan diuji\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode=None,  # Mode ini memungkinkan model untuk menghasilkan prediksi\n",
        "    shuffle=False  # Jangan acak urutan gambar\n",
        ")\n",
        "\n",
        "# Prediksi menggunakan model\n",
        "predictions = model.predict(test_generator)\n",
        "\n",
        "# Label kelas\n",
        "class_labels = list(train_generator.class_indices.keys())\n",
        "\n",
        "# Menampilkan hasil prediksi dengan nama file\n",
        "for i, prediction in enumerate(predictions):\n",
        "    gambar_filename = os.path.basename(test_generator.filenames[i])\n",
        "    predicted_class = class_labels[prediction.argmax()]\n",
        "    print(f\"{gambar_filename} : : {predicted_class}\")"
      ],
      "metadata": {
        "id": "DLUIlZpFVSiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebdce8c2-bb60-4631-f9c5-1f3d44fd4739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 28 images belonging to 7 classes.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "100k1.JPG : : 100k\n",
            "100k2.JPG : : 100k\n",
            "100k3.JPG : : 100k\n",
            "100k4.JPG : : 100k\n",
            "10k1.JPG : : 10k\n",
            "10k2.JPG : : 50k\n",
            "10k3.JPG : : 10k\n",
            "10k4.JPG : : 10k\n",
            "1k1.JPG : : 1k\n",
            "1k2.JPG : : 1k\n",
            "1k3.JPG : : 1k\n",
            "1k4.JPG : : 1k\n",
            "20k1.JPG : : 20k\n",
            "20k2.JPG : : 20k\n",
            "20k3.JPG : : 20k\n",
            "20k4.JPG : : 20k\n",
            "2k1.JPG : : 2k\n",
            "2k2.JPG : : 2k\n",
            "2k3.JPG : : 2k\n",
            "2k4.JPG : : 2k\n",
            "50k1.JPG : : 50k\n",
            "50k2.JPG : : 50k\n",
            "50k3.JPG : : 50k\n",
            "50k4.JPG : : 50k\n",
            "5k1.JPG : : 5k\n",
            "5k2.JPG : : 5k\n",
            "5k3.JPG : : 5k\n",
            "5k4.JPG : : 5k\n"
          ]
        }
      ]
    }
  ]
}